\documentclass{article}
\title{Education Project}
\author{Jared Wilber, Shannon Chang, Noura Kawa, Manuel Horta}
\date{December 5, 2016}

\usepackage{graphicx}
\usepackage[section]{placeins}
\graphicspath{ {../images/eda/} {../images/pca/} {../images/xgboost/}}

\begin{document}
\maketitle
\SweaveOpts{concordance=TRUE}


\begin{abstract}

Our client is a non-profit NGO with an interest in minority success. They want to donate money to schools that serve minorities well. We provide a method of choosing schools which are most likely to help minority students succeed by using the "collegescorecard" data and analysis of variables that are most significant for minorities.  
\end{abstract}

\section{Introduction}

Our Question: A minority-focused NGO wants to donate money to schools in an effort to help increase minority success. In order to do this, they need to identify those schools that "underserve" minority students. As they may have multiple schools in consideration, they'd also like a way to determine which of the underserving schools deserves funding.This is where we come in. We do the following:

\begin{itemize}
      \item Determine which universities "underserve" minorities (XGBOOST)
      \item Using a created BESTVALUE metric, rank universities based on how "good" they are.
\end{itemize}
\bigbreak

Thus, the NGO can employ us in the following 2 ways:

\begin{enumerate}
\item Given a school, predict whether or not it will underserve or overserves minorities (XGBOOST). 
\item Given multiple minority serving schools, determine which should give funding (BEST VALUE METRIC).
\end{enumerate}



Summary of our value: Given a list of schools, determine which schools underserve minorities. From these schools, rank them based on value.

\section{Analysis}

Our analysis consists of two primary objectives:
\begin{enumerate}
  \item Define model used to predict minority serving (XG-BOOST)
  \item Define metric used to rank schools (BEST VALUE RANKING)
\end{enumerate}  

\textbf{Train XG-Boost model}
Describe process (split into train/test, one-hot encoding, make numeric, etc.)
Return Model Accuracy

\medbreak

\textbf{Ranking schools}
We used lasso/xgboost feature importance to find the variables that US NEWS uses, then made a metric similar to theirs. Using this new BEST VALUE metric, we ranked our data.
         
\subsection{Data}

Because of fluctuations in data and the low volatility in year-to-year things (red-tape etc.), we opted to use the most recent dataset. In this way, we have access to more useful variables and also have a data set that is more likely to reflect present day. 

\medbreak
We want to identify those schools that overserve or underserve minorities. Our dataset lives in very high dimensions, so our first order of business is to reduce the dimensionality. This is important because the NGO would like to understand our methods, so our model must be interpretable. We broke this up into 4 steps:

\begin{enumerate}
  \item Hand-select important features from data. This yielded about 500 variables.
  \item Create our own variables, including BEST VALUE metric
  \item From these variables, we needed to select variables that were related to minorities. We determined these variables with FEATURE IMPORTANCE (LASSO/XGBOOST).
  \item Remove correlated variables
\end{enumerate}

Step 1 of the above is self-explanatory and we selected the following variables (column names):
\medbreak
\textbf{Variable names:}



\medbreak
Step 2 of the above is more involved. To create BEST VALUE, we first scraped the wh\_post (Washignton Post best school rankings)
for rankings. Once we had our ranked schools and appended them to our dataset, we used \linebreak LASSO/XGBOOST to pick those features that were associated with a school being ranked (we treat this as a proxy for quality). We then used these variables to build a BEST-VALUE metric.

insert feature importance


\medbreak
For step 3, we computed another feature importance metric, this time with our response being the minority served classification. We get rid of variables that don't contribute to our model. Because we are predicting discrete categories of minority serving, we use xgboost for feature importance.


\medbreak
Step 4 is to check for correlated variables and to remove them as well.

\begin{figure}[ht]
\includegraphics[width = 75mm]{corrplot-eda}
\end{figure}



\bigbreak
Finally, we have our dataset. Now our dataset is much smaller \textbf{and} more relevant to our goals. Thus, our data is less computationally expensive, has higher prediction power, and is more interpretable.

We end up with a data set consisting of 110 variables. These are then sorted by significance to minorities using xgboost.

\begin{figure}[h]
\includegraphics [width = 75mm]{feature-importance}
\caption{\textbf {Sorted significant variables.}}
\end{figure}

        

\subsection{Exploratory Data Analysis}


  Plot ideas: What they reveal
    
\begin{figure}[ht]
\includegraphics [width = 75mm]{hist-quality-index}
\caption{\textbf {Histogram of the Quality Index.}}
\end{figure}


\begin{figure}[ht]
\includegraphics [width = 75mm]{hist-best-value}
\caption{\textbf {Histogram of best value.}}
\end{figure}

\begin{figure}[ht]
\includegraphics [width = 75mm]{hist-above-median-minorities}
\caption{\textbf {Histogram of ABOVE\_MEDIAN\_MINORITIES.}}
\end{figure}

\begin{figure}[ht]
\includegraphics [width = 75mm]{boxplots-school-value-minority-Count-per-region}
\caption{\textbf {Boxplots of School Value by Minority Count per Region.}}
\end{figure}

\begin{figure}[ht]
\includegraphics [width = 75mm]{boxplots-minority-percentage-by-value-quartiles}
\caption{\textbf {Boxplots of Minority Percentage by Value Quartiles.}}
\end{figure}

\begin{figure}[ht]
\includegraphics [width = 75mm]{autoplot}
\caption{\textbf {IDK WHAT THIS IS.}}
\end{figure}

\begin{figure}[ht]
\includegraphics [width = 75mm]{phylo-fan}
\caption{\textbf {Phylo Tree.}}
\end{figure}



\begin{figure}[ht]
\includegraphics [width = 75mm]{ggdendrogram}
\caption{\textbf {HIERARCHICAL CLUSTERING.}}
\end{figure}

\begin{figure}[ht]
\includegraphics [width = 75mm]{scree-plot}
\caption{\textbf {Scree plot.}}
\end{figure}

\begin{figure}[ht]
\includegraphics [width = 75mm]{pca-best-value}
\caption{\textbf {PCA of QUALITY\_INDEX}}
\end{figure}

\begin{figure}[ht]
\includegraphics [width = 75mm]{pca-enrollment-rate-minority}
\caption{\textbf {PCA of Minority Enrollment Rate}}
\end{figure}



\begin{figure}[ht]
\includegraphics [width = 75mm]{feature-plot}
\caption{\textbf {Feature plot.}}
\end{figure}


\begin{figure}[ht]
\includegraphics [width = 75mm]{tsne-best-value}
\caption{\textbf {t-SNE 2D Embedding of School Quality.}}
\end{figure}

\begin{figure}[ht]
\includegraphics [width = 75mm]{tsne-above-median-minorities}
\caption{\textbf {t-SNE 2D Embedding of Minority Count.}}
\end{figure}

\section{Results}
feature importance
xgboost - find values for minorities, use them in model
bestvalue metric - use coefficient values from lasso on ranking to get weights + relevant columns
optimized-feature-importance

\begin{figure}[ht]
\includegraphics [width = 75mm]{feature-importance}
\caption{\textbf {Sorted significant variables.}}
\end{figure}


\begin{figure}[ht]
\includegraphics [width = 75mm]{optimized-feature-importance}
\caption{\textbf {Top 10 sorted significant variables.}}
\end{figure}

\begin{figure}[ht]
\includegraphics [width = 75mm]{best-features}
\caption{\textbf {Gain from features.}}
\end{figure}

\begin{figure}[ht]
\includegraphics [width = 75mm]{best-features-cumsum}
\caption{\textbf {Gain from features.}}
\end{figure}

feature importance
xgboost - find values for minorities, use them in model
bestvalue metric - use coefficient values from lasso on ranking to get weights + relevant columns


\section{Conclusions}
- Model accuracy
- whatever else



\bigbreak
analysis train xgboost model
move variables out before compiling - overfill hbox
step 2 and 3 plots?
results comments
conclusions


\end{document}





